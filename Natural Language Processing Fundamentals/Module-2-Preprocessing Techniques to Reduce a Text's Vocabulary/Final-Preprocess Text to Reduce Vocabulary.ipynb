{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be794ec38b087c396a34cdc610767fed",
     "grade": false,
     "grade_id": "cell-ed42372af67d33c2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Part Two of the Course Project\n",
    "\n",
    "\n",
    "In this part of the course project, you will complete the `Pipe` class so that it can be used to build custom preprocessing pipelines. The `Pipe` class has been partially completed already, but you will need to complete the class attributes, methods, and properties to make this class fully functional. Most of the solutions you will need to write are one-liners, but several may take a few lines.\n",
    "\n",
    "The class methods containing preprocessing code are exposed as properties (with `@property` decorator). The properties can be called without parenthesis, which is convenient and visually attractive. Every preprocessing step logs the task name and some basic stats to the dictionary `DStat`, which is stored internally in the instantiated `Pipe` object. So, if needed, one can evaluate the compression of the original document's lexicon at each step of the pipeline.\n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6401784e2ae9de2b13f096e2b44cb026",
     "grade": false,
     "grade_id": "cell-54974e91313b0ac2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "To complete this project, you will need to import the `nltk`, `pandas`, and `contractions` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a31daae34090b3c94e340b0ebd8e957",
     "grade": false,
     "grade_id": "cell-6d840d50e4a6b6b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
    "import nltk, pandas as pd, numpy.testing as npt, unicodedata, contractions, re\n",
    "from numpy.testing import assert_equal as eq\n",
    "import unittest\n",
    "from colorunittest import run_unittest\n",
    "_ = nltk.download(['omw-1.4','brown','wordnet','stopwords','averaged_perceptron_tagger'], quiet=True)\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61beab6ddb217b2415dc843d5656411c",
     "grade": false,
     "grade_id": "cell-6cd71f529610cfeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Background\n",
    "    \n",
    "This project requires an understanding of [classes](https://docs.python.org/3/tutorial/classes.html) in object-oriented programming. If you are familiar with this concept, you can skip this section.\n",
    "    \n",
    "In Module 1, you were introduced to the concept that everything in Python is an *object*. Recall that strings are objects with built-in *methods*, e.g., `.join()`, `.split()`, `isalpha()`. For any string, you can call `.split()` to get a list of strings split at whitespace. This is because all string objects are created from the same *class*, i.e., a blueprint for objects.\n",
    "    \n",
    "Creating an object, often referred to as an *instance*, from a class automatically calls the (`__init__`) method of that class. The initializer accepts `self`, which references the current instance, and other variables to set the class *attributes*, i.e., variables that are shared among all instances. When `self` is passed to other methods/properties **within** the class, they will have access to the variables stored at the class level. For example, if we create two instances, `a=Pipe(...)` and `b=Pipe(...)`, the `self` object for `a` can only access all variables created inside `a` and does not have access to any variables created inside `b`. This encapsulation functionality is very useful for classes. If you feel rusty about class definitions in Python, review your Python prerequisite material or numerous online resources on this topic.\n",
    "    \n",
    "Object-level variables are accessible via dot notation in Python. A dataframe `df` is an object-level variable accessible via `self.df` inside the object and `Pipe(...).df` outside of the object `Pipe()`. Methods can also be accessed in a similar manner by using `@property` decorator in the class. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7aee0dc3c82b9e90a72d33bf73892c57",
     "grade": false,
     "grade_id": "cell-90180d1615827506",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Your Tasks \n",
    "\n",
    "* **Task 1**: Initialize attributes  \n",
    "Initialize the class attributes (`self.LsWords`, `self.DStat`, `self.LsStep`) in `__init__()`.\n",
    "    \n",
    "*  **Task 2**: Format Output String \n",
    "<br>Complete the `Out()` method to format the output string.\n",
    "\n",
    "*  **Tasks 3 - 10**: String Preprocessing Methods \n",
    "<br> Complete the `Low()`, `NoNum()`, `Words()`, `Stop()`, `Norm()`, `Exp()`, `Stem()`, `Lem()` properties.\n",
    "    \n",
    "\n",
    "## Checking Your Work\n",
    "\n",
    "Test cases are provided below the project code cell. The `Pipe` class you'll be writing is complex and includes many tasks, so you may want to check whether some methods work before you have completed other methods. You can test select methods in the test cases without completing all of the methods in the class. The final text case will test the full functionality of the `Pipe` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "303a96bad279e9726a473429a50e161d",
     "grade": false,
     "grade_id": "cell-07f5e4a9f03fca75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Expected Functionality Examples\n",
    "\n",
    "### Example 1: \n",
    "  \n",
    "    >>> LsDoc = \"I enjoy learning NLP\".split()\n",
    "    >>> Pipe(LsDoc, SsStopWords='nltk').Low.Stop.Stem.Out()\n",
    "    \"enjoy learn nlp\"\n",
    " \n",
    "In this example, the following properties are applied sequentially to the list of strings in `LsDoc` [\"I\", \"enjoy\", \"learning\", \"NLP\"]:\n",
    "1. `Low`:  applies lower casing to each word\n",
    "1. `Stop`: removes stop words\n",
    "1. `Stem`: applies PorterStemmer to each word\n",
    "1. `Out()` method returns a preprocessed sentence `\"enjoy learn nlp\"`\n",
    " \n",
    "Notice how the preprocessing steps are stitched with a period (`.Low.Stop.Stem`), where each property returns a reference to the `self` object so that another step can be added.\n",
    "    \n",
    "### Example 2:\n",
    "\n",
    "    >>> LsDoc = \"We'rè fighting CÓVÍD-19 in 2020; ánd we've WON!\".split()\n",
    "    >>> pp = Pipe(LsDoc, SsStopWords='nltk', SsLex='nltk').Low.Norm.Exp.Stop.Lem.NoNum.Words\n",
    "    >>> pp.Out()\n",
    "    >>> pp.df\n",
    "\n",
    "The following data frame is printed with the step and the corresponding statistics. \n",
    "\n",
    "\n",
    "|.|Step|Words|Vocab|CorrVocab|\n",
    "|-|-|-|-|-|\n",
    "|0|Initialize|8|8|3|\n",
    "|1|Lower-case|8|8|3|\n",
    "|2|Normalize accented characters|8|8|5|\n",
    "|3|Contraction expansion|10|9|6|\n",
    "|4|Remove stopwords|4|4|1|\n",
    "|5|Lemmatize|4|4|1|\n",
    "|6|Remove numbers|4|4|2|\n",
    "|7|Remove non-word characters|4|4|2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef1fc587c0099f47dadd30624cca68ae",
     "grade": false,
     "grade_id": "cell-f6006dd88bec1076",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "# `Pipe` Class Code Cell\n",
    "\n",
    "The following cell contains the `Pipe` class you'll complete. Right now, each property in the class is folded  to make it easier for you to orient yourself to the class. Click the arrows to the left of the text to unfold the part of the class you want to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9705e33c8f8999246c540c2f9c42bb2d",
     "grade": false,
     "grade_id": "cell-821e0a4decc12113",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# COMPLETE THIS CELL\n",
    "class Pipe():\n",
    "    '''Pipe class exposes several common preprocessing steps as object properties/methods,\n",
    "    which can be stitched into desirable NLP pipelines using a object's dot notation. \n",
    "    For example, \n",
    "            Pipe(LsDoc).Low.Stop.Stem.Words.Out() takes LsDoc list of words,\n",
    "    and passes it through NLP sequence of steps:\n",
    "            lower-casing -> stop word removal -> stemming -> join words into a document\n",
    "    In the process, each method accumulates basic statistics from the current list of words'''\n",
    "\n",
    "    ### TASK 1: Attribute Initialization ###########################################\n",
    "    def __init__(self, LsWords=[], SsLex=set(), SsStopWords=set()) -> object:\n",
    "        '''Class constructor. Object-scope variables are initialized here. \n",
    "            Then we call AddStats with 'Initialize' argument to save current statistics for LsWords.\n",
    "        Input:\n",
    "            LsWords: List[str], string tokens of a document that needs preprocessing\n",
    "            SsLex:   Set[str] or 'nltk'. \n",
    "                A lexicon (set of lower-cased words) to be used for spell checking.\n",
    "                For 'nltk', we use the set of lower-case words from Brown corpus.\n",
    "            SsStopWords: Set[str] or 'nltk'. \n",
    "                A list of lower-case words to be used as stopwords.\n",
    "                For 'nltk', we use the set of NLTK English stopwords.\n",
    "        Returns: reference to self object    '''\n",
    "        # Ensure correct data structures are passed into the object's initialization method\n",
    "        assert isinstance(LsWords, list) or LsWords is None, f'LsWords must be a list, not a {type(LsWords)}'\n",
    "        assert isinstance(SsLex, set) or (SsLex=='nltk'), f'SsLex must be \"nltk\" or a set of lexicon words, not a {type(SsLex)}'\n",
    "        assert isinstance(SsStopWords, set) or (SsStopWords=='nltk'), f'SsStopWords must be \"nltk\" or a set of words, not a {type(SsStopWords)}'\n",
    "\n",
    "        # df stores preprocessing step name and associated statistics. \n",
    "        # We declare a blank object-level dataframe with 4 columns:\n",
    "        self.df = pd.DataFrame(columns = ['Step', 'Words', 'Vocab', 'CorrVocab'])\n",
    "        \n",
    "        # Save each __init__ input value to the object's variable with the same name.  \n",
    "        # Implement default cases for SsLex & SsStopWords as described in docstring above.\n",
    "\n",
    "        _ = nltk.download(['brown'], quiet=True)\n",
    "        Ss6 = {s.lower() for s in nltk.corpus.brown.words()}\n",
    "        \n",
    "        # No code changes are necessary for Task 1\n",
    "        # The print statements below are examples that may be helpful for troubleshooting\n",
    "        # You may uncomment them and use them, or use additional / different print statements\n",
    "        # If used, just be sure to re-comment when you are finished troubleshooting \n",
    "        # to reduce output\n",
    "        \n",
    "        #print(\"finished lower-casing of nltk corpus brown words\")\n",
    "        #print(\"first 10 of Ss6 {}\".format(Ss6[:10]))\n",
    "        #print(\"Ss6 length {}\".format(len(Ss6)))\n",
    "        \n",
    "        self.LsWords = LsWords\n",
    "        if SsLex =='nltk':\n",
    "            self.SsLex = Ss6\n",
    "            #print(\"using base nltk lexicon\")\n",
    "        else: \n",
    "            self.SsLex = SsLex\n",
    "            #print(\"using custom lexicon {}\".format(SsLex))\n",
    "        if SsStopWords =='nltk':\n",
    "            self.SsStopWords = set(stopwords.words('english'))\n",
    "            #print(\"using base nltk stopwords\")\n",
    "            #print(\"stopwords = {}\".format(self.SsStopWords))\n",
    "        else: \n",
    "            self.SsStopWords = SsStopWords\n",
    "            #print(\"using custom stopwords\")\n",
    "\n",
    "        self.AddStats('Initialize')     # Saves basic stats for LsWord\n",
    "        \n",
    "    ### TASK 2: Output ###########################################\n",
    "    def Out(self) -> str:\n",
    "        '''Use string's join method to concatenate words in self.LsWords, \n",
    "            separated by a single space. Before returning the string, \n",
    "            replace any instance of multi-whitespace with ' '.\n",
    "            Whitespace characters (space, \\t, \\r, \\n) are represented by \\s in regex.\n",
    "        Returns: a string of single-space-separated cleaned words\n",
    "        \n",
    "        For reference, review these pages in the course from Module 1:\n",
    "        - Preprocess Substrings with Operations\n",
    "        - Practice Preprocessing Substrings with Operations\n",
    "        - Overview of Regular Expressions\n",
    "        - Practice Using Simple Expressions'''\n",
    "        \n",
    "        # The return statement in this function is provided as a guide. \n",
    "        # You may uncomment and use if it fits your approach, or ignore it if not\n",
    "        # In other functions, return statements have been provided for you.\n",
    "        # You will perform operations on self.LsWords per the function specifications\n",
    "        # and then return the entire self object when complete\n",
    "        # If you are unfamiliar with this type of coding, you may refer to\n",
    "        # the AddStats function at the bottom of this cell as a reference\n",
    "        \n",
    "        # Once again, some sample print statements have been provided for this function which\n",
    "        # you may uncomment and use if you find them useful, and you may ignore them if not needed\n",
    "        # You can implement similar print statements in the other functions as well if desired\n",
    "        # Remember to comment out all print statements when you have finished troubleshooting\n",
    "        # in order to minimize unnecessary print output upon submission of your assignment\n",
    "        \n",
    "        #print(********)\n",
    "        #print(\"running the Out function\")\n",
    "        #print(********)\n",
    "        #print(\"self.LsWords starting point = {}\".format(self.LsWords))\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        return re.sub(r'\\s+', ' ', ' '.join(self.LsWords)).strip()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(********)\n",
    "        #print(\"modified self.LsWords = {}\".format(self.LsWords))\n",
    "        #print(********)\n",
    "        \n",
    "        #return self\n",
    "        \n",
    "                \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    ### TASK 3: Lowercase ###########################################\n",
    "    @property\n",
    "    def Low(self) -> object:\n",
    "        '''Applies lower casing to each word token in self.LsWords and \n",
    "            saves results back to self.LsWords.\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review the downloadable tool from Module 1:\n",
    "        - String Manipulation Methods'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        self.LsWords = [word.lower() for word in self.LsWords]\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Lower-case') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "        \n",
    "    ### TASK 4: Remove Digits ###########################################\n",
    "    @property\n",
    "    def NoNum(self) -> object:\n",
    "        ''' Use use re.sub() to remove all digits from strings in self.LsWords \n",
    "            and save results back to self.LsWords\n",
    "        In general, the impact of removal of numbers needs to be carefully investigated.\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course from Module 1:\n",
    "        - Overview of Regular Expressions\n",
    "        - Practice Using Simple Expressions'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LsWords = [re.sub(r'\\d+', '', word) for word in self.LsWords]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Remove numbers') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "        \n",
    "    ### TASK 5: Keep Only Word Characters ###########################################\n",
    "    @property\n",
    "    def Words(self) -> object:\n",
    "        '''Use re.sub() to keep word characters ('\\w': letters, numbers, underscore) and spaces\n",
    "            only in self.LsWords. Save results back to self.LsWords.\n",
    "        Note: Removing quotation marks impacts contraction expansion.\n",
    "        In general, the impact of removal of special characters needs to be carefully investigated.\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course from Module 1:\n",
    "        - Overview of Regular Expressions\n",
    "        - Practice Using Simple Expressions'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LsWords = [re.sub(r'[^\\w\\s]', '', word) for word in self.LsWords]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Remove non-word characters') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "        \n",
    "    ### TASK 6: Remove Stop Words ###########################################\n",
    "    @property\n",
    "    def Stop(self) -> object:\n",
    "        '''Remove stopwords self.LsWords and save back to self.LsWords.\n",
    "            Iterate over elements of self.LsWords and throw away those, \n",
    "            which are in self.SsStopWords regardless of letter casing.\n",
    "        Hint: lower-case words only when checking membership in self.SsStopWords\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course from Module 1:\n",
    "        - Removing Stop Words\n",
    "        - Remove Stop Words from a Document'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LsWords = [word for word in self.LsWords if word.lower() not in self.SsStopWords]\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Remove stopwords') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "\n",
    "    ### TASK 7: Normalization ###########################################\n",
    "    @property\n",
    "    def Norm(self) -> object:\n",
    "        '''Normalization of accented characters or diacritics. Each word in self.LsWords \n",
    "            needs to be deaccented using normalize(), encode() and decode() methods.\n",
    "            The list of normalized words is then saved back to self.LsWords\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course:\n",
    "        - Working with Characters\n",
    "        - Work with Characters to Standardize a Vocabulary'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LsWords = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore') for word in self.LsWords]\n",
    "\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Normalize accented characters') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "\n",
    "    ### TASK 8: Expand Contractions ###########################################\n",
    "    @property\n",
    "    def Exp(self) -> object:\n",
    "        '''Applies character expansion to self.LsWords and saves results back to self.LsWords.\n",
    "        1. Space-concatenate all tokens in self.LsWords.\n",
    "        2. Apply contractions.fix() method to the full string\n",
    "        3. Use split() to parse the pre-processed string back to list of tokens\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course:\n",
    "        - Expanding Contractions\n",
    "        - Modify and Add a Contraction Map'''\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.LsWords = contractions.fix(' '.join(self.LsWords)).split()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Contraction expansion') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "    \n",
    "    ### TASK 9: Stem ###########################################\n",
    "    @property\n",
    "    def Stem(self) -> object:\n",
    "        '''Porter Stemming of self.LsWords\n",
    "            Iterate over self.LsWords and stem each word using stem() method of pso object\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course:\n",
    "        - Stemming and Lemmatization\n",
    "        - Stem and Lemmatize a Document to Measure Vocabulary Quality'''\n",
    "        \n",
    "        pso = nltk.stem.PorterStemmer()       # instantiates Porter Stemmer object\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        \n",
    "        self.LsWords = [pso.stem(word) for word in self.LsWords]\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Stem') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "\n",
    "    ### TASK 10: Lemmatize ###########################################\n",
    "    @property\n",
    "    def Lem(self) -> object:\n",
    "        '''Wordnet Lemmatization of self.LsWords\n",
    "            Iterate over LTssWordTag and lemmatize each word using its \n",
    "            WordNet POS tag and wlo.lemmatize() method. The POS tagging and\n",
    "            tag conversion (from NLTK to WordNet tags) have been already implemented.\n",
    "            Caution: nltk.pos_tag() is designed to take sentence tokens, not large documents.\n",
    "        Example: \n",
    "            wlo.lemmatize('ran','v') returns 'run', but \n",
    "            wlo.lemmatize('ran','n') returns 'ran' (unintentionally)\n",
    "        Returns: reference to self object for continued chaining of properties \n",
    "        \n",
    "        For reference, review these pages in the course:\n",
    "        - Stemming and Lemmatization\n",
    "        - Stem and Lemmatize a Document to Measure Vocabulary Quality'''\n",
    "        \n",
    "        wlo = nltk.stem.WordNetLemmatizer()   # instantiates WordNet Lemmatizer object\n",
    "        WNTag = lambda t: t[0].lower() if t[0] in 'ARNV' else 'n'   # Converts NLTK POS Tag to WordNet POS Tag\n",
    "        # Create a list of tuples of words & their WordNet POS tags, \n",
    "        #    i.e. 'a' for adjectives, 'r' for adverbs, 'v' for verbs, 'n' for nouns and all else \n",
    "        LTssWordTag = [(word, WNTag(tag)) for word, tag in nltk.pos_tag(self.LsWords)]\n",
    "        \n",
    "        # YOUR CODE HERE \n",
    "        \n",
    "        self.LsWords = [wlo.lemmatize(word, tag) for word, tag in LTssWordTag]\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        return self.AddStats('Lemmatize') # finally, we save basic stats after this preprocessing\n",
    "\n",
    "\n",
    "    def AddStats(self, sTask='') -> object:\n",
    "        '''Object's preprocessing methods call AddStats() to save \n",
    "            basic word counts resulting from the NLP task.\n",
    "        Input: \n",
    "            sTask: string,a brief description of the task. Eg. 'Low', 'Stem', ...\n",
    "        Returns: reference to self object '''\n",
    "        # Append a row (sStep, nWords, nVocab, nCorrVocab) at the bottom of self.df, where\n",
    "        #   nWords =     count of words in self.LsWords\n",
    "        #   SsWords =    set of unique words from self.LsWords\n",
    "        #   nVocab =     count of words in SsWords\n",
    "        #   nCorrVocab = count of words in the intersection of SsWords and self.SsLex\n",
    "        \n",
    "        # Note: This function is complete as-is. No edits are needed for the assignment.\n",
    "        # This function is also not necessary to reference in the your code.\n",
    "        \n",
    "        SsWords = {s for s in self.LsWords}\n",
    "        self.df.loc[len(self.df)] = [sTask, len(self.LsWords), len(SsWords), len(SsWords.intersection(self.SsLex))]\n",
    "        return self     # Finally, return reference to the object itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Tests\n",
    "\n",
    "Here is a set of tests that evaluate whether initialization of `Pipe` class was implemented correctly.\n",
    "\n",
    "### Task 1: Object Initialization Tests\n",
    "* Methods required: `__init__`\n",
    "* **Note:** The following tests may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34066fc8089da873a6fd342055c07b0d",
     "grade": false,
     "grade_id": "2P_Pipe_Init_test_pre",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 11 tests in 71.681s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_05 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_06 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_07 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_08 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_09 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_10 (__main__.TestObjInitialization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "LsDoc = \"We'rè fighting CÓVÍD-19 in 2020; ánd we've WON!\".split()\n",
    "SsLex = {\"We'rè\", 'fighting', 'CÓVÍD-19'}\n",
    "SsStopWords = {'in', 'ánd'}\n",
    "@run_unittest\n",
    "class TestObjInitialization(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).LsWords[:3], [\"We'rè\", 'fighting', 'CÓVÍD-19'])\n",
    "    def test_01(self): eq (Pipe(LsDoc).SsLex, set())\n",
    "    def test_02(self): eq (Pipe(LsDoc).SsStopWords, set())\n",
    "    def test_03(self): eq (Pipe(LsDoc, SsLex=SsLex).LsWords[:3], [\"We'rè\", 'fighting', 'CÓVÍD-19'])\n",
    "    def test_04(self): eq (Pipe(LsDoc, SsLex=SsLex).SsLex, {'CÓVÍD-19', 'fighting', \"We'rè\"})\n",
    "    def test_05(self): eq (Pipe(LsDoc, SsLex=SsLex).SsStopWords, set())\n",
    "    def test_06(self): eq (sorted(Pipe(LsDoc, SsLex='nltk').SsLex)[:3], ['!', '$.027', '$.03'])\n",
    "    def test_07(self): eq (Pipe(LsDoc, SsStopWords=SsStopWords).LsWords[:3], [\"We'rè\", 'fighting', 'CÓVÍD-19'])\n",
    "    def test_08(self): eq (Pipe(LsDoc, SsStopWords=SsStopWords).SsLex, set())\n",
    "    def test_09(self): eq (Pipe(LsDoc, SsStopWords=SsStopWords).SsStopWords, {'ánd', 'in'})\n",
    "    def test_10(self): eq (sorted(Pipe(LsDoc, SsStopWords='nltk').SsStopWords)[:3], ['a', 'about', 'above'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Output Test \n",
    "\n",
    "These tests evaluate returned result of the initialized `Pipe` object.\n",
    "\n",
    "* Methods required: `__init__`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1e8c1dbde03c87ea2ad11c5c9fce5a6",
     "grade": true,
     "grade_id": "2P_Pipe_Out_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 1 test in 7.633s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestOutput) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestOutput(unittest.TestCase):\n",
    "    def test_00(self): npt.assert_equal (Pipe(LsDoc + ['  \\t\\n\\r!']).Out(), \"We'rè fighting CÓVÍD-19 in 2020; ánd we've WON! !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6055fe44bad2eed77eba2d2363025621",
     "grade": false,
     "grade_id": "cell-029bbcdc18e2e261",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 3: Lowercase Test\n",
    "* Methods required: `__init__`, `Low()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33d0d934a2938ecb0a7e570bdc95ca40",
     "grade": true,
     "grade_id": "2P_Pipe_LCase_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 1 test in 6.375s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestLowercase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestLowercase(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Low.Out(), \"we'rè fighting cóvíd-19 in 2020; ánd we've won!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eff55033adcd9029219c77f78113d05",
     "grade": false,
     "grade_id": "cell-d9f88996ccd1306b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 4: Number Removal Test\n",
    "* Methods required: `__init__`, `NoNum()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac6a16d7ba77c5999286801688ce0d26",
     "grade": true,
     "grade_id": "2P_Pipe_NoNum_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 1 test in 7.566s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestNumRemoval) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestNumRemoval(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).NoNum.Out(), \"We'rè fighting CÓVÍD- in ; ánd we've WON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dab5e581217bc972f574352139112ce5",
     "grade": false,
     "grade_id": "cell-99f51d07ee9df744",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 5: Word Filter Tests\n",
    "* Methods required: `__init__`, `Words()`, `NoNum()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e17132a5907347a57fff79d5c6ac281d",
     "grade": true,
     "grade_id": "2P_Pipe_Words_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 11.970s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestWordFilter) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestWordFilter) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestWordFilter(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Words.Out(), 'Werè fighting CÓVÍD19 in 2020 ánd weve WON')\n",
    "    def test_01(self): eq (Pipe(LsDoc).Words.NoNum.Out(), 'Werè fighting CÓVÍD in ánd weve WON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee26c53ca2966792f84548c1b4edace4",
     "grade": false,
     "grade_id": "cell-350d5d3a7d450f86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 6: Stopword Removal Tests\n",
    "* Methods required: `__init__`, `Stop()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44229342fc6b1a1dc6328604a36f8069",
     "grade": true,
     "grade_id": "2P_Pipe_Stopwords_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 18.053s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestStopwordRemoval) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestStopwordRemoval) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestStopwordRemoval) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestStopwordRemoval(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Stop.Out(), \"We'rè fighting CÓVÍD-19 in 2020; ánd we've WON!\")\n",
    "    def test_01(self): eq (Pipe(LsDoc, SsStopWords='nltk').Stop.Out(), \"We'rè fighting CÓVÍD-19 2020; ánd we've WON!\")\n",
    "    def test_02(self): eq (Pipe(LsDoc, SsStopWords={'ánd'}).Stop.Out(), \"We'rè fighting CÓVÍD-19 in 2020; we've WON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be473e109fd798e0c5f4011c2faec4aa",
     "grade": false,
     "grade_id": "cell-af47ab716cc1c2f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 7: Character Normalization Tests\n",
    "* Methods required: `__init__`, `Norm()`, `Stop()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "603be190b340ddb8fb8964c3c2467ab9",
     "grade": true,
     "grade_id": "2P_Pipe_Normzn_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 11.706s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestCharNormalization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestCharNormalization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestCharNormalization(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Norm.Out(), \"We're fighting COVID-19 in 2020; and we've WON!\")\n",
    "    def test_01(self): eq (Pipe(LsDoc, SsStopWords='nltk').Norm.Stop.Out(), \"We're fighting COVID-19 2020; we've WON!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9269c0b38be466a69c7ab10597009819",
     "grade": false,
     "grade_id": "cell-76647e0ff1e75041",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 8: Contraction Expansion Tests\n",
    "* Methods required: `__init__`, `Norm()`, `Exp()`, `Stop()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e94e9e4f3670d28ce08bc46e6201a4e",
     "grade": true,
     "grade_id": "2P_Pipe_Expand_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 19.218s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestContractionExpansion) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestContractionExpansion) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestContractionExpansion) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestContractionExpansion(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Exp.Out(), \"We'rè fighting CÓVÍD-19 in 2020; ánd we have WON!\")\n",
    "    def test_01(self): eq (Pipe(LsDoc).Norm.Exp.Out(), 'We are fighting COVID-19 in 2020; and we have WON!')\n",
    "    def test_02(self): eq (Pipe(LsDoc, SsStopWords='nltk').Norm.Exp.Stop.Out(), 'fighting COVID-19 2020; WON!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8340ff89d3766186fd184e6ae7a90ed4",
     "grade": false,
     "grade_id": "cell-f4333537c1e77430",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 9: Stemming Tests\n",
    "* Methods required: `__init__`, `Norm()`, `Exp()`, `Stem()`, `Words()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ba05124643da85517c951b8fe2cf0e",
     "grade": true,
     "grade_id": "2P_Pipe_Stem_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 4 tests in 28.049s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestStemming) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestStemming) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestStemming) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestStemming) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestStemming(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Stem.Out(), \"we'rè fight cóvíd-19 in 2020; ánd we'v won!\")\n",
    "    def test_01(self): eq (Pipe(LsDoc).Norm.Stem.Out(), \"we'r fight covid-19 in 2020; and we'v won!\")\n",
    "    def test_02(self): eq (Pipe(LsDoc).Norm.Exp.Stem.Out(), 'we are fight covid-19 in 2020; and we have won!')\n",
    "    def test_03(self): eq (Pipe(LsDoc).Norm.Exp.Stem.Words.Out(), 'we are fight covid19 in 2020 and we have won')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "215703b5eb7cb9938fb2337df6fb60c9",
     "grade": false,
     "grade_id": "cell-701dd99ff65031dd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 10: Lemmatization Tests\n",
    "* Methods required: `__init__`, `Norm()`, `Exp()`, `Words()`, `Low()`, `Lem()`, `Stop()`, `NoNum()`, `Out()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20d8825551706897770e2968b6e2837c",
     "grade": true,
     "grade_id": "2P_Pipe_Lemma_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 4 tests in 31.192s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestLemmatization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestLemmatization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestLemmatization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestLemmatization) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestLemmatization(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).Lem.Out(), \"We'rè fight CÓVÍD-19 in 2020; ánd we've WON!\")\n",
    "    def test_01(self): eq (Pipe(LsDoc).Norm.Exp.Words.Low.Lem.Out(), 'we be fight covid19 in 2020 and we have win')\n",
    "    def test_02(self): eq (Pipe(LsDoc, SsStopWords='nltk').Norm.Exp.Words.Low.Lem.Stop.Out(), 'fight covid19 2020 win')\n",
    "    def test_03(self): eq (Pipe(LsDoc, SsStopWords='nltk').Norm.Exp.Words.Low.Lem.Stop.NoNum.Out(), 'fight covid win')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "665ff6409d2c665ed4a0940a17104b16",
     "grade": false,
     "grade_id": "cell-ab4a34b5ab93cf07",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Task 11: df Tests\n",
    "* Methods required: all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99832fe016dc28b5208b6cc4d78ee718",
     "grade": true,
     "grade_id": "2P_Pipe_df_Test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 21.356s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.Testdf) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.Testdf) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.Testdf) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class Testdf(unittest.TestCase):\n",
    "    def test_00(self): eq (Pipe(LsDoc).df.values.ravel().tolist(), ['Initialize', 8, 8, 0])\n",
    "    def test_01(self): eq (Pipe(LsDoc, SsLex='nltk').df.tail(1).values.ravel().tolist()[1:], [8, 8, 3])  # Brown lexicon is used to match words in it\n",
    "    def test_02(self): eq (Pipe(LsDoc).Norm.Exp.df.tail(1).values.ravel().tolist()[1:], [10, 10, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f690d1bddcd7f6e86f5af1c4f8fbed90",
     "grade": false,
     "grade_id": "cell-3fe5507a78bda689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "# **Optional: Use Your Pipeline**\n",
    "\n",
    "Congratulations, you have just built a powerful preprocessing pipeline! Now, you can put this powerful pipeline machine to use on a larger corpus. First, load a text from Gutenberg library and run it through the cleaning pipeline to transform it into a list of words, which might represent the core meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b50f5d4635d05a0cf0491aff9f94ac6",
     "grade": false,
     "grade_id": "cell-6212d81db259a9e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "_ = nltk.download(['gutenberg'], quiet=True)\n",
    "LsBookWords = list(nltk.corpus.gutenberg.words('bryant-stories.txt')) #[:1000]\n",
    "sSampleText = nltk.corpus.gutenberg.raw('bryant-stories.txt')[:500] + '...\\n'\n",
    "print(sSampleText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "380175920abfa64c7cd5341296e11b17",
     "grade": false,
     "grade_id": "cell-b132d5dda6572d03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You can apply a sequence of preprocessing steps and output a dataframe with statistics at different steps of the pipeline. From what you have learned in this course, you might already have some expectations about the effectiveness of each step. For example, English language texts are less likely to benefit from removal of accent marks. You might expect that stemming and lemmatization would have the most dramatic drop in unique word count, but words produced from stemming may not be found in a dictionary, such as the lexicon created from NLTK's Brown Corpus. If you change the order of the steps, the counts are likely to change as well. In particular, special character normalization may remove quotation marks, which are needed for the contraction expansion to identify and fix contractions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88fb53329fe605624065fc5d17c3901f",
     "grade": false,
     "grade_id": "cell-59447eaf6dcceb9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%time pp = Pipe(LsBookWords, SsStopWords='nltk', SsLex='nltk').Low.Norm.Exp.Words.Stem.Stop.NoNum\n",
    "pp.Out()[:500]\n",
    "pp.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8544afaa77a7b478a2aafdde788a8f3e",
     "grade": false,
     "grade_id": "cell-ef19ac567735d05c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Further Exploration with Your Pipeline\n",
    "\n",
    "Investigate the original and clean document. How would you change the pipeline order to have the fewest unique and total words, but to have greatest overlap with Brown lexicon? Can you think of any other cleaning steps that might be useful for this pipeline?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
