{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part Three of the Course Project**\n",
    "\n",
    "In this part of the course project, you will apply the skills you have practiced in this module to complete functions in a preprocessing pipeline for tagging and parsing text. You will use each function individually to investigate a mix of large and small documents from several quotes about learning, as well as from a corpus of inaugural speeches given by American Presidents.  \n",
    "\n",
    "The preprocessing pipеline for tagging and parsing (i.e. tokenizing) a document you will create follows this workflow: \n",
    " \n",
    "$$\\newcommand{\\t}{\\texttt} \n",
    "\\newcommand{\\u}[2]{\\underset{\\texttt{#1}}{\\text{#2}}}\n",
    "\\newcommand{\\a}[2]{\\underset{\\text{#1}\\atop\\text{#2}}{\\longrightarrow}}\n",
    "\\t{sDoc}\\a{parse}{sentences}\n",
    "\\t{LsSents}\\a{parse}{words}\n",
    "\\t{LLsWords}\\a{Penn POS}{tagging}\n",
    "\\t{LLTsWordPOST}\\a{specific}{task}\n",
    "\\begin{cases} \n",
    "    \\t{chunk phrases} \\\\\n",
    "    \\t{WordNet POS tags}\\rightarrow \\t{WordNet word lemmas} \\\\\n",
    "    \\t{dependency tree (constituency tree can also be created)} \\\\\n",
    "\\end{cases}$$\n",
    " \n",
    "where \n",
    " \n",
    "1. `sDoc` is a string document with at least one sentence\n",
    "    1. Ex: a string with 2 sentences, `'I do. We go.'`\n",
    "1. `LsSents` is a list of string sentences\n",
    "    1. Ex: `['I do.', 'We go.']`\n",
    "1. `LLsWords` is a list of lists of words of sentences\n",
    "    1. Ex: `[['I','do','.'],['We','go','.']]`\n",
    "1. `LLTsWordPOST` is a list of lists of tuples of word & Penn POS tag pairs\n",
    "    1. Ex: `[[('I','PRP'),('do','VBP'),('.','.')],  [('We','PRP'),('go', 'VBP'),('.', '.')]]`\n",
    "1. Wordnet lemmatizer uses WordNet POS tags: `'a'`:adjective, `'n'`:noun (and is the default), `'r'`:adverb, `'v'`:verb\n",
    "    1. Ex: `[[('I','n'),('do','v'),('.','n')],  [('We','n'),('go', 'v'),('.', 'n')]]`\n",
    "\n",
    "    \n",
    "Note that the pipeline does not include sentence tokenization because the `spacy` library does sentence tokenization implicitly, so we can feed a whole document to it for processing.\n",
    "\n",
    "**Once you have completed this pipeline, you will be equipped to extract phrases, lemmas, and dependencies based on POS tags from the Penn and WordNet tagsets using the `nltk` and `spacy` libraries.**\n",
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Reset the Python environment to clear it of any previously loaded variables, functions, or libraries. Then, import the libraries and data sets you will need to complete this part of the course project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61d3ef7f04f9ca5ddfdd176b201b7625",
     "grade": false,
     "grade_id": "3P_import_read_only",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
    "import nltk, pandas as pd, spacy, numpy.testing as npt\n",
    "from nltk.corpus import inaugural\n",
    "from typing import List, Tuple, Set\n",
    "import unittest\n",
    "from colorunittest import run_unittest\n",
    "_ = nltk.download(['omw-1.4','wordnet', 'inaugural', 'averaged_perceptron_tagger', 'punkt', 'tagsets'], quiet=True);\n",
    "pd.set_option('max_rows', 10, 'max_columns', 100, 'max_colwidth', 100, 'precision', 2)\n",
    "eq, aeq = npt.assert_equal, npt.assert_almost_equal\n",
    "\n",
    "# Dictionary with epic quotes about education and learning:\n",
    "DsEdu = {'Albert Einstein':     \"Education is what remains after one has forgotten what one has learned in school.\",\n",
    "         'Albert Einstein(2)':  \"It’s not that I’m so smart, it’s just that I stay with problems longer.\",\n",
    "         'B. B. King':          \"The beautiful thing about learning is that no one can take it away from you.\",\n",
    "         'Benjamin Franklin':   \"An investment in knowledge pays the best interest.\",\n",
    "         'Sydney J. Harris':    \"The whole purpose of education is to turn mirrors into windows.\",\n",
    "         'Nelson Mandela':      \"Education is the most powerful weapon which you can use to change the world.\",\n",
    "         'Dorothy Parker':      \"The cure for boredom is curiosity. There is no cure for curiosity.\",\n",
    "         'Mahatma Gandhi':      \"Live as if you were to die tomorrow. Learn as if you were to live forever.\"}\n",
    "\n",
    "LLsPIWords = inaugural.sents('1841-Harrison.txt') # list of lists of words from Presidential Inaugurations (PI)\n",
    "LsPISents = [' '.join(words) for words in inaugural.sents('1841-Harrison.txt')]  # list of sentences from PI \n",
    "sPIDoc = inaugural.raw('1841-Harrison.txt') \n",
    "\n",
    "print(f'LLsPIWords =', str(LLsPIWords[:1])[:100])   # list of lists of sentence word strings\n",
    "print(f'LsPISents =', str(LsPISents[:1])[:100])     # list of sentence strings\n",
    "print(f'sPIDoc =', sPIDoc[:100])                    # string document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use the [Penn Treebank POS tag set](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), let's review the full list of tags here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7511b8d1b48da28e65807cad1b6f603",
     "grade": false,
     "grade_id": "3P_DTagSet_read_only",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset()  # prints all tags with descriptions\n",
    "pd.set_option('max_rows', 100, 'display.max_colwidth', 0)\n",
    "DTagSet = nltk.data.load('help/tagsets/upenn_tagset.pickle')  # dictionary of POS tags\n",
    "pd.DataFrame(DTagSet, index=['Definition', 'Examples']).T.sort_index().reset_index().rename(columns={'index':'Tag'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Pipeline\n",
    "\n",
    "    \n",
    "The series of functions that you'll complete to create the pipeline are below. You'll work through the pipeline step by step. Each function accomplishes a portion of the pipeline. Use the title of each function or group of functions to determine which part of the pipeline you're accomplishing as you complete each function.\n",
    "\n",
    "### Function 1: Parse `sDoc` into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfb1f98b5943501e88a481ebe83b965e",
     "grade": false,
     "grade_id": "3P_Doc2Sents_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def Doc2Sents(sDoc='I like NLP. NLP is fun.') -> List[str]:\n",
    "    '''Use nltk.sent_tokenize() to parse sDoc into a list of string sentences'''\n",
    "    return sent_tokenize(sDoc)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a88485b542bbde9cb80be5cc5159445",
     "grade": true,
     "grade_id": "3P_Doc2Sents_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 0.002s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestDoc2Sents) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestDoc2Sents) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestDoc2Sents) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestDoc2Sents(unittest.TestCase):\n",
    "    def test_00(self): eq(Doc2Sents(), ['I like NLP.', 'NLP is fun.'])\n",
    "    def test_01(self): eq(Doc2Sents(DsEdu['Benjamin Franklin']), ['An investment in knowledge pays the best interest.'])\n",
    "    def test_02(self): eq(Doc2Sents(DsEdu['Mahatma Gandhi']), ['Live as if you were to die tomorrow.', 'Learn as if you were to live forever.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: Parse Sentence into a List of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9e180a94576260e084798bfc36b18b",
     "grade": false,
     "grade_id": "3P_Sent2Words_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def Sent2Words(sSent='I like NLP') -> List[str]:\n",
    "    '''Use nltk.word_tokenize() to parse a single sentence into a list of words'''\n",
    "    return word_tokenize(sSent)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a3d17cc3433cfa7534def5366c199f1",
     "grade": true,
     "grade_id": "3P_Sent2Words_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 0.003s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestSent2Words) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestSent2Words) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestSent2Words(unittest.TestCase):\n",
    "    def test_00(self):  eq(Sent2Words(), ['I', 'like', 'NLP'])\n",
    "    def test_01(self): eq(Sent2Words(DsEdu['Benjamin Franklin']), \\\n",
    "        ['An','investment','in','knowledge','pays','the','best','interest', '.'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: POS Tag Each Sentence with UPenn Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c32b1fe762a7ca35d3ce59c3655cb8c",
     "grade": false,
     "grade_id": "3P_POST_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def POST(LsWords=['I','like','NLP']) -> List[Tuple[str]]:\n",
    "    ''' Use nltk.pos_tag() with default parameters to tag words in a sentence.\n",
    "    Return: list of tuples in the form (word string, Penn POS tag string)'''\n",
    "    return pos_tag(LsWords)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee3d0f8ce9c8a5319501dfd601d52074",
     "grade": true,
     "grade_id": "3P_POST_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 0.364s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestPOST) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestPOST) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestPOST(unittest.TestCase):\n",
    "    def test_00(self): eq(POST(), [('I', 'PRP'), ('like', 'VBP'), ('NLP', 'NNP')])\n",
    "    def test_01(self): eq(' '.join([tag for word, tag \\\n",
    "        in POST(Sent2Words(DsEdu['Mahatma Gandhi']))]), \\\n",
    "            'NNP IN IN PRP VBD TO VB NN . NNP IN IN PRP VBD TO VB RB .')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 4: Create Wordnet POS Tags from UPenn Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e50df5fd71f12f0fea10ebc3d8bbaf1b",
     "grade": false,
     "grade_id": "3P_WN_POST_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def WN_POST(LTsPOST=[('I', 'PRP'), ('like', 'VBP'), ('NLP', 'NNP')]) -> List[Tuple[str]]:\n",
    "    '''Replace Penn POS tags with WordNet POS tags. \n",
    "    Rules: tags 'a','v','r' replace any Penn tags starting with the same letters. \n",
    "    Tag 'n' replaces all other tags.\n",
    "    LTsPOST: list of tuples in the form (word string, Penn POS tag string)\n",
    "    Return: list of tuples in the form (word string, WordnNet POS tag string)     '''\n",
    "    result = []\n",
    "    for word, penn_tag in LTsPOST:\n",
    "        if penn_tag.startswith('JJ'):\n",
    "            result.append((word, 'a'))\n",
    "        elif penn_tag.startswith('VB'):\n",
    "            result.append((word, 'v'))\n",
    "        elif penn_tag.startswith('RB'):\n",
    "            result.append((word, 'r'))\n",
    "        else:\n",
    "            result.append((word, 'n'))\n",
    "    return result\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a522bfc0aab39de5b382f69dce77720d",
     "grade": true,
     "grade_id": "3P_WN_POST_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 0.008s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestWN_POST) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestWN_POST) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestWN_POST) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestWN_POST(unittest.TestCase):\n",
    "    def test_00(self): eq(WN_POST(), [('I', 'n'), ('like', 'v'), ('NLP', 'n')])\n",
    "    def test_01(self): eq(''.join([tag for word, tag in WN_POST(POST(Sent2Words(DsEdu['Albert Einstein'])))]), 'nvnvnnvvnnvvnnn')\n",
    "    def test_02(self): eq(''.join([tag for word, tag in WN_POST(POST(Sent2Words(DsEdu['Dorothy Parker'])))]), 'nnnnvnnnvnnnnn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Pipeline Tasks\n",
    "\n",
    "Now that you've completed many of the precursor steps in your pipeline, you're ready to use the preprocessed text to perform specific tasks. \n",
    "\n",
    "### Function 5: Lemmatize Wordnet-tagged Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11d2cead34880b63a8cf706d52fcca2f",
     "grade": false,
     "grade_id": "3P_Lemmas_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def Lemmas(LsWN_POST=[('I', 'n'), ('liked', 'v'), ('NLP', 'n')]) -> List[str]:\n",
    "    ''' Use nltk's WordNet lemmatizer with default parameters to convert \n",
    "            a list of WordNet-tagged words to a list of corresponding lemmas. '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word, pos_tag in LsWN_POST:\n",
    "        if pos_tag == 'n':\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "        elif pos_tag == 'v':\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "        elif pos_tag == 'a':\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet.ADJ)\n",
    "        elif pos_tag == 'r':\n",
    "            lemma = lemmatizer.lemmatize(word, wordnet.ADV)\n",
    "        else:\n",
    "            lemma = word\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db6d3407087f7002e58902c38370543e",
     "grade": true,
     "grade_id": "3P_Lemmas_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 3 tests in 3.710s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestLemmas) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestLemmas) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestLemmas) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestLemmas(unittest.TestCase):\n",
    "    def test_00(self): eq(Lemmas(),['I', 'like', 'NLP'])\n",
    "    def test_01(self): eq(Lemmas(WN_POST(POST(Sent2Words(DsEdu['Benjamin Franklin'])))), \\\n",
    "        ['An', 'investment', 'in', 'knowledge', 'pay', 'the', 'best', 'interest', '.'])\n",
    "    def test_02(self): eq(' '.join(Lemmas(WN_POST(POST(Sent2Words(DsEdu['Albert Einstein']))))), \\\n",
    "        'Education be what remain after one have forget what one have learn in school .')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chunking**\n",
    "\n",
    "Next you will apply regex parsers to extract chunk phrases from POS tagged sentences. The following function,  `ChunkNTag()`, is already defined for you. It retrieves labeled chunks from the `nltk` tree structure. Use it to define and explore your own chunk definitions with POS tags. See examples below the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4c77cc4682da1144e2ab08406a16785",
     "grade": false,
     "grade_id": "3P_ChunkNTag_read_only",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb phrases in \"I like NLP\":\t [('like', 'VP')]\n",
      "Original quote:\t Education is what remains after one has forgotten what one has learned in school.\n",
      "Tagged words:\t [('Education', 'NN'), ('is', 'VBZ'), ('what', 'WP'), ('remains', 'VBZ'), ('after', 'IN'), ('one', 'CD'), ('has', 'VBZ'), ('forgotten', 'VBN'), ('what', 'WP'), ('one', 'CD'), ('has', 'VBZ'), ('learned', 'VBN'), ('in', 'IN'), ('school', 'NN'), ('.', '.')]\n",
      "Verb phrases:\t [('has forgotten', 'VP'), ('has learned', 'VP')]\n",
      "Wh-word phrases\t [('what remains', 'WP'), ('what one has learned', 'WP')]\n",
      "Noun phrases:\t [('Education', 'NP'), ('school', 'NP')]\n",
      "In-NN phrases:\t [('in school', 'NP0')]\n",
      "CD+VP phrases:\t [('one has', 'CD VP'), ('one has', 'CD VP')]\n"
     ]
    }
   ],
   "source": [
    "def ChunkNTag(LTsPOST=[('I','PRP'),('like','VB'),('NLP','NN')], sGrammar=\"VP: {<V.*>+}\") -> List[Tuple[str]]:\n",
    "    '''Given a POS tagged sentence, extracts chunk-tagged phrased based on grammar specification.\n",
    "    Builds a shallow NLTK tree structure and retrieves chunks under the root of this tree, \n",
    "        which is the full sentence with a tag 'S'\n",
    "    Input:\n",
    "        LTsPOST: Penn POS-tagged words as a list of tuples of strings in the form (word, tag)\n",
    "        sGrammar: chunk grammar as a regex pattern string.\n",
    "    Return: list of tuples of strings of chunk phrase & tag pair '''\n",
    "    ChunkTree = nltk.RegexpParser(sGrammar).parse(LTsPOST)  # chunk parser returns an NLTK's Tree structure\n",
    "    # In double-loop below, we convert each subtree to a list of tuples with chunk label and its phrase\n",
    "    return [(' '.join(leaf[0] for leaf in tree.leaves()), tree.label()) for tree in ChunkTree.subtrees() if tree.label()!='S']\n",
    "\n",
    "print('Verb phrases in \"I like NLP\":\\t', ChunkNTag())\n",
    "print('Original quote:\\t',DsEdu['Albert Einstein'])\n",
    "print('Tagged words:\\t',  POST(Sent2Words(DsEdu['Albert Einstein'])))\n",
    "print('Verb phrases:\\t',  ChunkNTag(POST(Sent2Words(DsEdu['Albert Einstein'])), sGrammar=\"VP:{<VBZ><VBN>}\"))\n",
    "print('Wh-word phrases\\t',ChunkNTag(POST(Sent2Words(DsEdu['Albert Einstein'])), sGrammar=\"WP:{<WP><CD>?<V..>*}\"))\n",
    "print('Noun phrases:\\t',  ChunkNTag(POST(Sent2Words(DsEdu['Albert Einstein'])), sGrammar=\"NP: {<NN>}\"))\n",
    "print('In-NN phrases:\\t', ChunkNTag(POST(Sent2Words(DsEdu['Albert Einstein'])), sGrammar=\"NP0: {<IN><NN.*>}\"))\n",
    "print('CD+VP phrases:\\t', ChunkNTag(POST(Sent2Words(DsEdu['Albert Einstein'])), sGrammar=\"CD VP:{<CD><VB.>}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 6: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ccc620d33f9269b5a607529a8a017b3",
     "grade": false,
     "grade_id": "3P_Chunk_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Chunk(LTsPOST=[('I','PRP'),('like','VB'),('NLP','NN')], sGrammar=\"VP: {<V.*>+}\", sExtractTag='VP') -> List[str]:\n",
    "    '''Given a POS tagged sentence, use ChunkNTag() to extract a list \n",
    "       of chunk phrases only, filtered by sExtractTag.\n",
    "    Input:\n",
    "        LTsPOST, sGrammar: same arguments as in ChunkNTag()\n",
    "        sExtractTag: chunk tag string identifying chunk phrases to keep in the returned list.\n",
    "    Return: list of string chunk phrases corresponding to the specified sExtractTag. '''\n",
    "    chunked_phrases = ChunkNTag(LTsPOST, sGrammar)\n",
    "    filtered_phrases = [phrase for phrase, tag in chunked_phrases if tag == sExtractTag]\n",
    "    return filtered_phrases\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21b4d1b998381613cc8d1305d7dcf4c4",
     "grade": true,
     "grade_id": "3P_Chunk_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 8 tests in 0.010s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_05 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_06 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_07 (__main__.TestChunk) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "LTsPOST = POST(Sent2Words(DsEdu['Albert Einstein']))\n",
    "@run_unittest\n",
    "class TestChunk(unittest.TestCase):\n",
    "    def test_00(self): eq(Chunk(LTsPOST, sGrammar=\"VP:{<V.*>+}\", sExtractTag='VP'), ['is','remains','has forgotten','has learned'])\n",
    "    def test_01(self): eq(Chunk(LTsPOST, sGrammar=\"VP:{<V.*>+}\", sExtractTag='NP'), [])\n",
    "    def test_02(self): eq(Chunk(LTsPOST, sGrammar=\"NP:{<NN.*>+}\", sExtractTag='NP'), ['Education','school'])\n",
    "    def test_03(self): eq(Chunk(LTsPOST, sGrammar=\"NP:{<NN.*>+}\", sExtractTag='VP'), [])\n",
    "    def test_04(self): eq(Chunk(LTsPOST, sGrammar=\"WP:{<WP><CD>?<V..>*}\", sExtractTag='WP'), ['what remains','what one has learned'])\n",
    "    def test_05(self): eq(Chunk(LTsPOST, sGrammar=\"CD VP:{<CD><VB.>}\", sExtractTag='CD VP'), ['one has','one has'])\n",
    "    def test_06(self): eq(Chunk(LTsPOST, sGrammar=\"VP:{<V.*>+}\\nNP:{<NN.*>+}\", sExtractTag='NP'), ['Education', 'school'])\n",
    "    def test_07(self): eq(Chunk(LTsPOST, sGrammar=\"VP:{<V.*>+}\\nNP:{<NN.*>+}\", sExtractTag='VP'), ['is', 'remains', 'has forgotten', 'has learned'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 7: Chunking with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "313b880f7407e022fc242b697169f269",
     "grade": false,
     "grade_id": "3P_GetNP1_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetNP1(LTsPOST=[('I','PRP'),('like','VB'),('NLP','NN')]) -> List[str]:\n",
    "    ''' Use Chunk() to find chunk noun phrases (NP1) with\n",
    "        at most 1 determiner followed by at most 1 adjective \n",
    "        followed by at least one noun in any form.\n",
    "    Input:\n",
    "        LTsPOST: Penn POS-tagged words as a list of tuples of strings in the form (word, tag)\n",
    "    Returns a list of chunk noun phrases from the given sentence  '''\n",
    "    sGrammar = \"NP1: {<DT>?<JJ>?<NN.*>+}\"\n",
    "    np1_phrases = Chunk(LTsPOST, sGrammar, sExtractTag=\"NP1\")\n",
    "    return np1_phrases\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1f17d0ed72fdc5d8674e6ab1ab9d6d1",
     "grade": true,
     "grade_id": "3P_GetNP1_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 5 tests in 0.022s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestGetNP1) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestGetNP1) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestGetNP1) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestGetNP1) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestGetNP1) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestGetNP1(unittest.TestCase):\n",
    "    def test_00(self): eq(GetNP1(POST(Sent2Words(DsEdu['Albert Einstein']))), ['Education', 'school'])\n",
    "    def test_01(self): eq(GetNP1(POST(Sent2Words(DsEdu['B. B. King']))), ['The beautiful thing', 'learning', 'no one'])\n",
    "    def test_02(self): eq(GetNP1(POST(Sent2Words(DsEdu['Benjamin Franklin']))), ['An investment', 'knowledge', 'interest'])\n",
    "    def test_03(self): eq(GetNP1(POST(Sent2Words(DsEdu['Sydney J. Harris']))), ['The whole purpose', 'education', 'mirrors', 'windows'])\n",
    "    def test_04(self): eq(GetNP1(POST(Sent2Words(DsEdu['Albert Einstein(2)']))), ['m', 'problems'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 8: Chunking with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84059b547d4c3956339c6ac5008ee4cf",
     "grade": false,
     "grade_id": "3P_GetNP2_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetNP2(LTsPOST=[('I','PRP'),('like','VB'),('NLP','NN')]) -> List[str]:\n",
    "    ''' Use Chunk() to find chunk noun phrases (NP2) with\n",
    "        at exactly 1 determiner followed by at most 1 adjective followed by at least one noun in any form.\n",
    "    Input:\n",
    "        LTsPOST: Penn POS-tagged words as a list of tuples of strings in the form (word, tag)\n",
    "    Returns a list of chunk noun phrases from the given sentence that have  '''\n",
    "    sGrammar = \"NP2: {<DT><JJ>?<NN.*>+}\"\n",
    "    np2_phrases = Chunk(LTsPOST, sGrammar, sExtractTag=\"NP2\")\n",
    "    return np2_phrases\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c21e40296d4e57d3e009ffe1a7442f88",
     "grade": true,
     "grade_id": "3P_GetNP2_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 5 tests in 0.040s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestGetNP2) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestGetNP2) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestGetNP2) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestGetNP2) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestGetNP2) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestGetNP2(unittest.TestCase):\n",
    "    def test_00(self): eq(GetNP2(POST(Sent2Words(DsEdu['Albert Einstein']))), [])\n",
    "    def test_01(self): eq(GetNP2(POST(Sent2Words(DsEdu['B. B. King']))), ['The beautiful thing', 'no one'])\n",
    "    def test_02(self): eq(GetNP2(POST(Sent2Words(DsEdu['Benjamin Franklin']))), ['An investment'])\n",
    "    def test_03(self): eq(GetNP2(POST(Sent2Words(DsEdu['Sydney J. Harris']))), ['The whole purpose'])\n",
    "    def test_04(self): eq(GetNP2(POST(Sent2Words(DsEdu['Albert Einstein(2)']))), [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 9: Chunking with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c6f9fd0af3e9d86634a75e339126b47",
     "grade": false,
     "grade_id": "3P_GetNP3_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def GetNP3(LTsPOST=[('I','PRP'),('like','VB'),('NLP','NN')]) -> List[str]:\n",
    "    ''' Use Chunk() to find chunk noun phrases (NP3) with\n",
    "        exactly 1 noun (in any form) exactly 1 verb (in any form, with POS tag starting with VB).\n",
    "    Input:\n",
    "        LTsPOST: Penn POS-tagged words as a list of tuples of strings in the form (word, tag)\n",
    "    Returns a list of chunk noun phrases from the given sentence that have  '''\n",
    "    sGrammar = \"NP3: {<NN.*><VB.*>}\"\n",
    "    np3_phrases = Chunk(LTsPOST, sGrammar, sExtractTag=\"NP3\")\n",
    "    return np3_phrases\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15fd72eeb62725c49671d490d1a58b5f",
     "grade": true,
     "grade_id": "3P_GetNP3_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 5 tests in 0.033s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestGetNP3) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestGetNP3) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestGetNP3) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestGetNP3) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestGetNP3) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestGetNP3(unittest.TestCase):\n",
    "    def test_00(self): eq(GetNP3(POST(Sent2Words(DsEdu['Albert Einstein']))), ['Education is'])\n",
    "    def test_01(self): eq(GetNP3(POST(Sent2Words(DsEdu['B. B. King']))), ['learning is'])\n",
    "    def test_02(self): eq(GetNP3(POST(Sent2Words(DsEdu['Benjamin Franklin']))), ['knowledge pays'])\n",
    "    def test_03(self): eq(GetNP3(POST(Sent2Words(DsEdu['Sydney J. Harris']))), ['education is'])\n",
    "    def test_04(self): eq(GetNP3(POST(Sent2Words(DsEdu['Albert Einstein(2)']))), [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 10: Chunk Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45e64499b4056709a2c12d39f1f02ee9",
     "grade": false,
     "grade_id": "3P_FreqNP1Phrase_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def FreqNP1Phrase(LLTsPOST=[[('I','PRP'),('like','VBP'),('math','NN')]], \n",
    "                  nMinFreq=1, nMinChar=1) -> List[Tuple[str,int]]:\n",
    "    '''Use collections.Counter().most_common() and GetNP1() to find \n",
    "        the most frequent NP1 chunks in LLTsPOST and of restricted length.\n",
    "    Input:\n",
    "        LLTsPOST: list of lists of tuples of strings with (word, Penn POS tag) format\n",
    "        nMinFreq: min frequency for the chunk phrases\n",
    "        nMinChar: the minimum allowed number of characters in the chunk phrase\n",
    "    Return: list of tuples of (phrase string, count integer)  '''\n",
    "    np1_chunks = []\n",
    "    for sentence in LLTsPOST:\n",
    "        np1_chunks.extend(GetNP1(sentence))\n",
    "    counter = Counter(np1_chunks)\n",
    "    filtered_chunks = [(phrase, count) for phrase, count in counter.items() \n",
    "                        if count >= nMinFreq and len(phrase) >= nMinChar]\n",
    "    return sorted(filtered_chunks, key=lambda x: x[1], reverse=True)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c415ce447751861316a96b51848dadb2",
     "grade": true,
     "grade_id": "3P_FreqNP1Phrase_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 5 tests in 1.236s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestFreqNP1Phrase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestFreqNP1Phrase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_02 (__main__.TestFreqNP1Phrase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_03 (__main__.TestFreqNP1Phrase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_04 (__main__.TestFreqNP1Phrase) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "LTsEdu = [POST(Sent2Words(s)) for s in Doc2Sents(' '.join(DsEdu.values()).lower())]\n",
    "@run_unittest\n",
    "class TestFreqNP1Phrase(unittest.TestCase):\n",
    "    def test_00(self): eq(FreqNP1Phrase(LTsEdu[0:1]), [('education', 1), ('school', 1)])\n",
    "    def test_01(self): eq(FreqNP1Phrase(LTsEdu[0:2]), [('education', 1), ('school', 1), ('m', 1), ('problems', 1)])\n",
    "    def test_02(self): eq(FreqNP1Phrase(LTsEdu[0:5], 2), [('education', 2)])\n",
    "    def test_03(self): eq(FreqNP1Phrase(LTsEdu, 2), [('education', 3), ('curiosity', 2)])\n",
    "    def test_04(self):\n",
    "        LTsPI_POST = [POST(Sent2Words(s)) for s in LsPISents] # let's test on presidential inaugural speech\n",
    "        LTsnPIOut = [('the Constitution',31), ('the people', 24), ('the Government', 12), ('the character', 10), ('the Executive', 10)]\n",
    "        eq(FreqNP1Phrase(LTsPI_POST, nMinFreq=10, nMinChar=10), LTsnPIOut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dependency Parsing**\n",
    " \n",
    "Finally, you will extract the dependency tree from a sentence. In this case, SpaCy does most of the parsing and tagging work for you, you just have to extract specific components from its `nlp` object.\n",
    "\n",
    "### Function 11: Spacy Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c7720ae42b2412d3f77cec6d1414b54",
     "grade": false,
     "grade_id": "3P_DTRoot_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")   # SpaCy's text-processing pipeline object for English\n",
    "def DTRoot(sSent='I like NLP') -> List[str]:\n",
    "    '''Find all roots of SpaCy's dependency trees from sentence.\n",
    "    Note: for complex sentences SpaCy returns multiple trees with correpsonding roots.\n",
    "    Input: sSent: sentence string\n",
    "    Return: list of roots (strings) '''\n",
    "    doc = nlp(sSent)\n",
    "    roots = [token.text for token in doc if token.dep_ == 'ROOT']\n",
    "    return roots\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61247345639e6f9452956c9675b61192",
     "grade": true,
     "grade_id": "3P_DTRoot_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 0.191s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestDTRoot) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestDTRoot) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestDTRoot(unittest.TestCase):\n",
    "    def test_00(self):eq(DTRoot(), ['like'])\n",
    "    def test_01(self): eq([DTRoot(s) for s in DsEdu.values()], \\\n",
    "        [['is'],['’s'],['is'],['pays'],['is'],['is'],['is', 'is'],['Live', 'Learn']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 12: Root Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee445419b616fd7ad55bade2c7b6f834",
     "grade": false,
     "grade_id": "3P_FreqDTRoot_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def FreqDTRoot(sDoc='I like NLP', nMinFreq=1, nMinChar=2) -> List[Tuple[str,int]]:\n",
    "    '''Use collections.Counter.most_common() and DTRoot() to find \n",
    "        most frequent roots from a document string with multiple sentences.\n",
    "        Filter out roots shorter than nMinChar.\n",
    "    Input: \n",
    "        sDoc: a string with multiple sentences, which need to be split with Doc2Sents()\n",
    "        nMinFreq: min frequency for the root word\n",
    "        nMinChar: the minimum allowed number of characters in the root word\n",
    "    Return: list of tuples of (root string, count integer) format   '''\n",
    "    doc = nlp(sDoc)\n",
    "    roots = []\n",
    "    for sent in doc.sents:\n",
    "        roots.extend(DTRoot(sent.text))\n",
    "    counter = Counter(roots)\n",
    "    filtered_roots = [(root, count) for root, count in counter.items()\n",
    "                      if count >= nMinFreq and len(root) >= nMinChar]\n",
    "    return sorted(filtered_roots, key=lambda x: x[1], reverse=True)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "747425d2fba6ac1c45431e5dc6f25c5c",
     "grade": true,
     "grade_id": "3P_FreqDTRoot_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 2 tests in 10.891s\n",
      "\n",
      "\u001b[1m\u001b[34mOK\u001b[0m\n",
      "test_00 (__main__.TestFreqDTRoot) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "test_01 (__main__.TestFreqDTRoot) ... \u001b[1m\u001b[34mok\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN CELL TO TEST YOUR CODE\n",
    "@run_unittest\n",
    "class TestFreqDTRoot(unittest.TestCase):\n",
    "    def test_00(self): eq(FreqDTRoot(' '.join(DsEdu.values()), 2), [('is', 6)])\n",
    "    def test_01(self): eq(FreqDTRoot(sPIDoc, 3, 6), \\\n",
    "        [('become', 6), ('appear', 4), ('observed', 3), ('appears', 3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
