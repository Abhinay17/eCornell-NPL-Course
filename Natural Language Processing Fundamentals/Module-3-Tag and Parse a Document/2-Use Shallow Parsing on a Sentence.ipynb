{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review**\n",
    " \n",
    "Clear the Python environment of any previously loaded variables, functions, and libraries. Then, import the libraries needed to complete the code Professor Melnikov presented in the video. \n",
    "\n",
    "SpaCy library uses a [`en_core_web_sm`](https://spacy.io/models/en#en_core_web_sm), a small set of pre-processing functions for English text (pre-trained on WWW content)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "# !pip -q install spacy>=3.0.0 > log               # quietly upgrade SpaCy, which uses `en_core_web_sm` model\n",
    "# !python -m spacy download en_core_web_sm > log   # download small English model\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
    "import nltk, spacy, pandas as pd\n",
    "from spacy import displacy\n",
    "print('SpaCy version:', spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Review**\n",
    "\n",
    "In this notebook you will use chunking to shallow parse sentences.\n",
    "\n",
    "## Chunking\n",
    "\n",
    "[Chunking](https://www.nltk.org/book_1ed/ch07.html) is a type of **partial** or **shallow** parsing that retrieves only flat, non-overlapping segments from a sentence rather than a hierarchically parsed representation. A simple bracketing notation can be used to indicate the start, end, and type of each chunk. \n",
    "\n",
    "## Using Regex Notation to Chunk a Sentence\n",
    "\n",
    "You can use regex rules to tell Python how to use POS tags to parse a sentence by defining rules for different types of phrases. For example: \n",
    "\n",
    "1. The code `\"VP: {<V.*>+}\"` indicates a verb phrase (marked as `VP`), which must have at least one `<V.*>` tag (indicated by a `+`). \n",
    "  * `V.` allows any 2-letter tags starting with `V`, such as `VB` (base form) and `VP` (verb phrase).\n",
    "  * `V.*` allows at least 2 letter tags starting with `V`, such as `VBD` (past tense), `VBG` (gerund/present participle), `VBN` (past participle), `VBP` and `VBZ`. \n",
    "1. `\"V2V: {<V.*> <TO> <V.*>}\"` is a chunk we named `V2V` which must start with a verb in any form, followed by `\"to\"`, followed by another verb in any form. For example:\n",
    "\n",
    "  $$\\newcommand{\\u}[2]{\\underset{\\text{#1}}{\\,\\text{ #2 }\\,}} \"\\u{PRP}{We}\\underbrace{\\u{VBP}{dress}\\u{TO}{to}\\u{VB}{impress}}_{\\text{V2V}}.\"\\,\\,\\,\\,\\,,\\,\\,\\,\\,\\, \"\\u{PRP}{He}\\underbrace{\\u{VBD}{loved}\\u{TO}{to}\\u{VB}{work}}_{\\text{V2V}}.\"$$\n",
    "  \n",
    "1. The code `\"NP: {<JJ>?<NN.*>}\"` finds noun phrases (marked as `NP`), which may start with a single adjective (identified with a tag `JJ`) and are followed by any noun with a tag `NN.*`, which includes `NNS`, `NNP`, and `NNPS`. For example, this sentence has several noun phrases matching the regex pattern:\n",
    "\n",
    "$$\"\\underbrace{\\u{NNP}{Alice}}_{\\text{NP}} \\u{VBD}{had}\\u{DT}{a} \\underbrace{\\u{NN}{dog}}_{\\text{NP}},\\u{DT}{a} \\underbrace{\\u{JJ}{brown}\\u{NN}{fox}}_{\\text{NP}},\\u{CC}{and}\\underbrace{\\u{JJ}{many}\\u{NNS}{cats}}_{\\text{NP}}.\"$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Chunks with a Pre-Determined Grammar\n",
    "\n",
    "To identify chunks with pre-determined grammar, we first need to identify POS tags for all elements of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('An', 'DT'), ('independent', 'JJ'), ('newspaper', 'NN'), (',', ','), ('The', 'DT'), ('Cornell', 'NNP'), ('Daily', 'NNP'), ('Sun', 'NNP'), (',', ','), ('was', 'VBD'), ('founded', 'VBN'), ('by', 'IN'), ('William', 'NNP'), ('Ballard', 'NNP'), ('Hoyt', 'NNP'), ('in', 'IN'), ('1880', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sNews = 'An independent newspaper, The Cornell Daily Sun, was founded by William Ballard Hoyt in 1880.'\n",
    "LsNewsTokens = nltk.word_tokenize(sNews)   # parse a sentence into a list of word tokens\n",
    "LTsNewsTags = nltk.pos_tag(LsNewsTokens)   # POS-tag all word tokens\n",
    "print(LTsNewsTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following noun phrase looks for the pattern `determiner + adjective + noun`. \n",
    "\n",
    "1. The `'?'` allows at most one determiner. \n",
    "1. The `'<JJ>*'` allows none or more adjectives, which are identified by POS tag `'JJ'`\n",
    "1. The `'<NN.*>'` allows different nouns (`'NN'`, `'NNP'`, etc.), containing 2 or more letters (starting with `'NN'`) in their POS tag.\n",
    "\n",
    "Use this pattern to perform shallow parsing on the sentence above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP An/DT independent/JJ newspaper/NN)\n",
      "  ,/,\n",
      "  (NP The/DT Cornell/NNP Daily/NNP Sun/NNP)\n",
      "  ,/,\n",
      "  was/VBD\n",
      "  founded/VBN\n",
      "  by/IN\n",
      "  (NP William/NNP Ballard/NNP Hoyt/NNP)\n",
      "  in/IN\n",
      "  1880/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sNounPhraseGrammar = 'NP: {<DT>?<JJ>*<NN.*>*}' # determiner + adjective + noun\n",
    "cp = nltk.RegexpParser(sNounPhraseGrammar)     # initialize the parser with a grammar definition\n",
    "treeNews = cp.parse(LTsNewsTags)               # parse a sentence (words+tags) into a shallow tree of words and chunks\n",
    "print(treeNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm correctly identified the name of the paper and its founder. The output is presented in the form of a schematic tree diagram, where `'S'` indicates the full sentence (as a root of the tree). Chunking is a type of shallow parsing, because this tree has a single level below its root. The root is linked to leaves (in their original order), some of which are individual word tokens and others are chunks. The chunk is made up of leaves that matched the grammar pattern.\n",
    "\n",
    "## Working with the Parsed Tree\n",
    "\n",
    "The parsed tree is cumbersome to work with programmatically. So, if you need a list of identified chunks, you can iterate over tree leaves and collect only those that do not have a label `'S'`, which indicates that the leaf is an individual word token and is not a chunk phrase. Investigate the `Tree2Chunks()` function below, which prints only the chunk phrases and their tags (=labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('An independent newspaper', 'NP'), ('The Cornell Daily Sun', 'NP'), ('William Ballard Hoyt', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "def Tree2Chunks(tree):\n",
    "    JoinLeaves = lambda tree: (' '.join(leaf[0] for leaf in tree.leaves()), tree.label()) # chunk phrase and its label\n",
    "    return [JoinLeaves(tree) for tree in tree.subtrees() if tree.label()!='S']\n",
    "print(Tree2Chunks(treeNews)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks with Larger Grammar\n",
    "\n",
    "The following (somewhat sophisticated) chunking pattern searches for different types of grammatical phrases.\n",
    "It leaves out `\"The\"` determiner from the newspaper's name by design. All grammars are labeled with names that are allowed to repeat. So, verb phrases are identified by `'VP'`. Noun phrases are identified with `'NP'`. \n",
    "\n",
    "The basic grammar syntax is as following:\n",
    "\n",
    "1. Each chunk phrase definition:\n",
    "    1. has a tag(=label), a colon, and a definition wrapped in curly brackets. E.g. `'VP: {...}'`\n",
    "        1. different definitions can have the same tags (for example, there are several combinations that qualify as a verb phrase.)\n",
    "    1. contains Penn Tree POS tags in angle brackets. E.g. `'<NN>'`\n",
    "    1. respects regex syntax\n",
    "1. A grammar is a collection of newline-separated phrase definitions (`'VP:{...}\\nNP:{...}'`)\n",
    "\n",
    "Take some time to investigate chunk definitions. You can try this with your own sentences or chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  An/DT\n",
      "  independent/JJ\n",
      "  (NP newspaper/NN)\n",
      "  ,/,\n",
      "  The/DT\n",
      "  (NP Cornell/NNP Daily/NNP Sun/NNP)\n",
      "  ,/,\n",
      "  (VP was/VBD founded/VBN)\n",
      "  by/IN\n",
      "  (NP William/NNP Ballard/NNP Hoyt/NNP)\n",
      "  in/IN\n",
      "  1880/CD\n",
      "  ./.)\n",
      "[('newspaper', 'NP'), ('Cornell Daily Sun', 'NP'), ('was founded', 'VP'), ('William Ballard Hoyt', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "sGrammar=r\"\"\"\n",
    "\n",
    "    # Verb Phrase Definitions\n",
    "    \n",
    "    VP: {<V.*>+}\n",
    "    VP: {<ADJ_SIM><V_PRS>}\n",
    "    VP: {<ADJ_INO><V.*>}\n",
    "    VP: {<V_PRS><N_SING><V_SUB>}\n",
    "    VP: {<N_SING><V_.*>}\n",
    "    \n",
    "    \n",
    "    # Noun Phrase Definitions\n",
    "\n",
    "    NP: {<N.*><PRO>}\n",
    "    NP: {<ADJ.*>?<N.*>+ <ADJ.*>?}\n",
    "    NP: {<N_SING><ADJ.*><N_SING>}\n",
    "    \n",
    "    \n",
    "    # Determiner followed by a NP (Noun Phrase, as defined above)\n",
    "\n",
    "    DNP: {<DET><NP>}\n",
    "    \n",
    "    \n",
    "    # EDIT -- THIS NEEDS TO BE DEFINED (Oleg?)\n",
    "    \n",
    "    PP: {<P>*}\n",
    "    PP: {<P><N_SING>}\n",
    "    PP: {<ADJ_CMPR><P>}\n",
    "    PP: {<ADJ_SIM><P>}\n",
    "    \n",
    "    \n",
    "    # Noun Phrase followed by a DNP (Determiner followed by a Noun Phrase, as defined above)\n",
    "    \n",
    "    DDNP: {<NP><DNP>}\n",
    "    \n",
    "    \n",
    "    # EDIT -- PP NEEDS TO BE DEFINED (Oleg?) and then this finished out\n",
    "    \n",
    "    NPP: {<PP><NP>+}\n",
    "    \"\"\"\n",
    "cp = nltk.RegexpParser(sGrammar)\n",
    "treeNews2 = cp.parse(LTsNewsTags)\n",
    "print(treeNews2)\n",
    "print(Tree2Chunks(treeNews2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking with SpaCy\n",
    "\n",
    "[SpaCy](https://spacy.io/usage/models#quickstart) is a powerful NLP package, which uses sophisticated pre-trained models to provide rich meta information about sentences and documents. Here we load one small (`'sm'`) English (`'en'`) model, which is trained on web content. A string sentence is then wrapped into an `nlp` object, which provides access to underlying text, noun phrases, labels, root noun of each phrase, and much more. SpaCy provides superior quality parsers and labels. Its only drawback is that it's still slower than comparable NLTK tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Noun phrase</th>\n",
       "      <th>label</th>\n",
       "      <th>Root noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An independent newspaper</td>\n",
       "      <td>NP</td>\n",
       "      <td>newspaper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cornell Daily Sun</td>\n",
       "      <td>NP</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Ballard Hoyt</td>\n",
       "      <td>NP</td>\n",
       "      <td>Hoyt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Noun phrase label  Root noun\n",
       "0  An independent newspaper    NP  newspaper\n",
       "1     The Cornell Daily Sun    NP        Sun\n",
       "2      William Ballard Hoyt    NP       Hoyt"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # text-processing pipeline object for English\n",
    "doc = nlp(sNews)                   # A sequence of Token objects\n",
    "LTsNP = [(c.text, c.label_, c.root.text) for c in doc.noun_chunks]\n",
    "pd.DataFrame(LTsNP, columns=['Noun phrase', 'label', 'Root noun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy also has a [displacy](https://spacy.io/usage/visualizers) visualizer object used to render noun phrases in color, with tags. Below is our sentence, where organization, person, and date are identified. It is amazing that SpaCy was able to \"understand\" the role of these phrases without us providing any additional definitions or context. Perhaps, SpaCy’s model has already \"seen\" these phrases while training on the web corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">An independent newspaper, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Cornell Daily Sun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", was founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    William Ballard Hoyt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1880\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 2px solid #606366; background: transparent;\">\n",
    "\n",
    "# **Optional Practice**\n",
    "\n",
    "Now you will practice on a few related tasks.\n",
    "    \n",
    "If you need to recall the definitions of Penn Tree tags, use this code:\n",
    "\n",
    "    pd.set_option('max_rows', 100, 'display.max_colwidth', 0)\n",
    "    DTagSet = nltk.data.load('help/tagsets/upenn_tagset.pickle')  # dictionary of POS tags\n",
    "    pd.DataFrame(DTagSet, index=['Definition', 'Examples']).T.sort_index().reset_index().rename(columns={'index':'Tag'})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
    "sPython = \"Python was conceived in the late 1980s by Guido van Rossum \\\n",
    "at Centrum Wiskunde & Informatica (CWI) in the Netherlands \\\n",
    "as a successor to ABC programming language, which was inspired by SETL,\\\n",
    "capable of exception handling and interfacing with the Amoeba operating system. \\\n",
    "Its implementation began in December 1989. Van Rossum shouldered sole responsibility \\\n",
    "for the project, as the lead developer, until 12 July 2018, when he announced his \\\n",
    "\\\"permanent vacation\\\" from his responsibilities as Python's Benevolent Dictator For Life, \\\n",
    "a title the Python community bestowed upon him to reflect his long-term commitment \\\n",
    "as the project's chief decision-maker. In January 2019, active Python core \\\n",
    "developers elected a 5-member \\\"Steering Council\\\" to lead the project. \\\n",
    "As of 2021, the current members of this council are Barry Warsaw, Brett Cannon, \\\n",
    "Carol Willing, Thomas Wouters, and Pablo Galindo Salgado.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTsPythonTags = nltk.pos_tag(nltk.word_tokenize(sPython))   # POS-tag all word tokens\n",
    "print(LTsPythonTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work through these tasks, check your answers by running your code in the *#check solution here* cell, to see if you’ve gotten the correct result. If you get stuck on a task, click the See **solution** drop-down to view the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "In `sPython` find noun phrases which start with one determiner, followed by one adjective, followed by one noun of any type. For example, your grammar should identify the following chunks:\n",
    "\n",
    "    [('the late 1980s', 'NP'), ('the current members', 'NP')]\n",
    "    \n",
    "<b>Hint:</b> Try a grammar <code>'NP: {&lt;DT&gt;&lt;JJ&gt;&lt;NN.*&gt;}'</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "    <pre>\n",
    "cp = nltk.RegexpParser('NP: {&lt;DT&gt;&lt;JJ&gt;&lt;NN.*&gt;}')\n",
    "print(Tree2Chunks(cp.parse(LTsPythonTags)))\n",
    "    </pre>\n",
    "    </details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "In `sPython` find number phrases which start with `IN`-tagged word (preposition or subordinating conjunction), followed by a proper singular noun, followed by a numeral or cardinal. For example, your grammar should find the following chunks labeled as `Date`.\n",
    "\n",
    "    [('in December 1989', 'Date'), ('In January 2019', 'Date')]\n",
    "    \n",
    " <b>Hint:</b> Try a grammar <code>'Date: {&lt;IN&gt;&lt;NNP&gt;&lt;CD&gt;}'</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "    <pre>\n",
    "cp = nltk.RegexpParser('Date: {&lt;IN&gt;&lt;NNP&gt;&lt;CD&gt;}')\n",
    "print(Tree2Chunks(cp.parse(LTsPythonTags)))\n",
    "    </pre>\n",
    "    </details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "In `sPython` find entity phrases, which have two or more nouns in any form (`NN`, `NNP`, `NNS`, ...). For example, your grammar should retieve the following chunks labeled as `Entity`:\n",
    "\n",
    "    [('Guido van Rossum', 'Entity'), ('Centrum Wiskunde', 'Entity'), , ('ABC programming language', 'Entity'), ...]\n",
    "    \n",
    " <b>Hint:</b> You can use two nouns with the third one as optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "    <pre>\n",
    "# Solution 1:\n",
    "cp = nltk.RegexpParser('Entity:{&lt;NN.*>&lt;NN.*>&lt;NN.*>*}')\n",
    "print(Tree2Chunks(cp.parse(LTsPythonTags)))\n",
    "# Solution 2:\n",
    "cp = nltk.RegexpParser('Entity:{&lt;NN.*>&lt;NN.*>+}')\n",
    "print(Tree2Chunks(cp.parse(LTsPythonTags)))\n",
    "    </pre>\n",
    "    </details> \n",
    "</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "In `sPython` find chunk phrases which contain noun phrases `<DT>?<JJ><NN>` or `<VB.*><NN.*>`. For example, your grammer should extract the following chunks:\n",
    "\n",
    "    [('sole responsibility', 'NP'), ('permanent vacation', 'NP'), ...]\n",
    "    \n",
    "<b>Hint:</b> When combining phrase definitions, remember to separate these with a newline character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=#606366>\n",
    "    <details><summary><font color=#b31b1b>▶ </font>See <b>solution</b>.</summary>\n",
    "    <pre>\n",
    "cp = nltk.RegexpParser('NP: {&lt;DT>?&lt;JJ>&lt;NN>} \\nVP: {&lt;VB.*>&lt;NN.*>}')\n",
    "print(Tree2Chunks(cp.parse(LTsPythonTags)))\n",
    "    </pre>\n",
    "    </details> \n",
    "</font>\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
